{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjAdqrPKq9n/g2cXZVCyBt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niklaust/Deep_Learning/blob/main/PyTorch_for_Deep_Learning_notebook_of_nikluast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reference**\n",
        "Ian Pointer. (2019). *Programming PyTorch For Deep Learning Creating and Deploying Deep Learning Application*. O'Reilly"
      ],
      "metadata": {
        "id": "7sa8DyqUmEMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "github:niklaust"
      ],
      "metadata": {
        "id": "IgxpYe30mJdq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "start 20230220"
      ],
      "metadata": {
        "id": "RVRNs0e5mHr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><center><b>Programming PyTorch for Deep Learning</b></center></h1>"
      ],
      "metadata": {
        "id": "wuUZl1kumLIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What is Deep Learning?**"
      ],
      "metadata": {
        "id": "FLnNS88noYF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A machine learning technique** that **uses multiple and numerous layers of nonlinear** transforms to progressively extract features from raw input\n",
        "\n",
        "**A technique to solve problems** by providing the inputs and desired outputs and letting the computer find the solution, normally **using a neural network.**"
      ],
      "metadata": {
        "id": "HKyDFGn7oe3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <center><b>Chapter 1. Getting Started with PyTorch</b></center>"
      ],
      "metadata": {
        "id": "dSdKfmwKRIJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPU** is The **heart** of every deep learning box. It is going to **power the majority of PyTorch's calculations.**"
      ],
      "metadata": {
        "id": "ZXrpje1cSHPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Getting Start with PyTorch!** "
      ],
      "metadata": {
        "id": "rUr-hmpFWSBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.rand(2, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyIgYdeoWcBF",
        "outputId": "84e592ed-b29c-4d0d-cb71-c6570ddb5d69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "tensor([[0.8933, 0.0782],\n",
            "        [0.8609, 0.6029]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tensors**"
      ],
      "metadata": {
        "id": "Y_2-2GbbW9nK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **tensor is both a container for numbers as well as a set of rules** that define transformations between tensors that produce new tensors.\n",
        "\n",
        "It's easier to think **tensors as multidimensional arrays.**"
      ],
      "metadata": {
        "id": "nnLKPjivXDI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([[0,0,1],[1,1,1],[0,0,0]])\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsNDwSXhX-cK",
        "outputId": "3bf1828b-f0d2-420d-d649-88a1c440ebe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 1],\n",
              "        [1, 1, 1],\n",
              "        [0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change an element in a tnsor by using standard Python indexing:"
      ],
      "metadata": {
        "id": "eR9ItT5GYJKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x[0][0] = 5\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7UqJaWRYT5x",
        "outputId": "2a9bcbc5-aae0-400c-e26a-91cc526b599a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[5, 0, 1],\n",
              "        [1, 1, 1],\n",
              "        [0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.zeros(2,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HO3viBHHYcQi",
        "outputId": "a582e020-11df-43af-c550-868cd400d05f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0.],\n",
              "        [0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.ones(3,3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hFbuNfbYhws",
        "outputId": "c2281073-f474-49af-ef44-ed19e245a5ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.ones(1,2) + torch.ones(1,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tNlKw7OYn5r",
        "outputId": "84a54246-9a42-42a7-f100-9cfd7aa82136"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2., 2.]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`item`: pull out the value "
      ],
      "metadata": {
        "id": "JSSmXQ5KhRAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "torch.rand(1).item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "On458QXNg1cZ",
        "outputId": "72b04c86-cb17-4bc1-9abd-f0e0b59ff919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8822692632675171"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`to`: copy between devices "
      ],
      "metadata": {
        "id": "mUTN2b2yhLpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "XGxk_JNSiLOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cpu_tensor = torch.rand(2)\n",
        "cpu_tensor.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bgrE8X-hHrx",
        "outputId": "f487afe4-cb98-4038-cc7b-aea2d939341d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_tensor = cpu_tensor.to(\"cuda\")\n",
        "gpu_tensor.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSledL6FiPVb",
        "outputId": "c0f2deab-170d-4484-ffff-8c13d252bc42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tensor Operations**"
      ],
      "metadata": {
        "id": "kzoeMd4virvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "a = torch.rand(2,2)\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAe9z9e7jKbq",
        "outputId": "81c78925-b2c7-45f9-b896-5285aad737d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8823, 0.9150],\n",
              "        [0.3829, 0.9593]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.max())             # torch.rand(2,2).max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AY94wtXqi_ol",
        "outputId": "bd58d085-bd50-4cb6-f7f1-b29000b0595a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.9593)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.max().item())      # torch.rand(2,2).max().item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyzVyI0jjD7p",
        "outputId": "bfd60534-7af0-4769-c7d1-67e479c37e67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9593056440353394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`type`: to see the element type in the tensor\n",
        "`dtype`: to change the type of a tensor"
      ],
      "metadata": {
        "id": "88iMXEO_lBo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "long_tensor = torch.tensor([[0,0,1],[1,1,1],[0,0,0]])\n",
        "long_tensor.type()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "J1A7kuX3kl_D",
        "outputId": "11052c58-eb15-430d-ed55-8796fe0d2595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'torch.LongTensor'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "float_tensor = torch.tensor([[0,0,1],[1,1,1],[0,0,0]]).to(dtype=torch.float32)\n",
        "float_tensor.type()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nAGqJKickw7J",
        "outputId": "c3fbd4c8-d7d2-4bfd-8377-41e6fdcdd954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'torch.FloatTensor'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "appended underscore `_`: save memory, look to see if an in-place function is defined "
      ],
      "metadata": {
        "id": "MDrF3YQ-lotZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "random_tensor = torch.rand(2,2)\n",
        "random_tensor.log2()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3-Dmy3NljN5",
        "outputId": "f026d49d-f672-47fb-af14-5c8f8dd532d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1807, -0.1282],\n",
              "        [-1.3851, -0.0599]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_tensor.log2_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2TNrjv6mJnR",
        "outputId": "1510467b-3328-4e71-84ad-93292ea14b6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1807, -0.1282],\n",
              "        [-1.3851, -0.0599]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**reshape a tensor:**\n",
        "\n",
        "`view`:  **operates as a view on the original tensor**, so if the underlying data is changed, the view will change too (and vice versa). However, it can throw errors if the required view is not contiguous. It doesn't share the same block of memory it would occupy if a new tensor of  the required shape was created from scratch, you have to call `tensor.contiguous()` before you can use `view()`.\n",
        "\n",
        "`reshape`: to reshape a tensor"
      ],
      "metadata": {
        "id": "Xwp1EOOSmWbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "flat_tensor = torch.rand(784)\n",
        "print(flat_tensor.shape)                         # 1*28*28 = 784\n",
        "viewed_tensor = flat_tensor.view(1, 28, 28)\n",
        "print(viewed_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdLDyNQPmWEZ",
        "outputId": "7ce7b176-6f87-4236-8daf-f86f0a7c0dd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([784])\n",
            "torch.Size([1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "reshaped_tensor = flat_tensor.reshape(1, 28, 28)\n",
        "print(reshaped_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrmeGlVlm2EB",
        "outputId": "1d96e1a3-ffbb-48fe-c24e-30f95b7acaa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the reshpaed tensor's shape has to have the same number of \n",
        "# total elements as the original. \n",
        "\n",
        "try:\n",
        "  flat_tensor.reshape(3, 28, 28)\n",
        "except:\n",
        "  print(\"RuntimeError: shape '[3, 28, 28]' is invalid for input of size 784\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADo85p0inbmZ",
        "outputId": "8288860c-a030-4f08-9e6c-6f0d441f5aa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RuntimeError: shape '[3, 28, 28]' is invalid for input of size 784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rearrange the dimensions of a tensor.**\n",
        "\n",
        "`permute` : rearrange the dimensions of a tensor\n",
        "\n",
        "You will likely come across this with images, which often are stored as `[height, width, channel]` tensors, but PyTorch prefers to deal with these in a `[channel, height, width]` you can use `permute()` to deal with these in a fairly straightforward manner:"
      ],
      "metadata": {
        "id": "skJhMzUXpAuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "hwc_tensor = torch.rand(640, 480, 3)            # [height, width, channel]\n",
        "print(hwc_tensor.shape)\n",
        "chw_tensor = hwc_tensor.permute(2,0,1)          # [channel, height, width]\n",
        "print(chw_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npshDGfNprhx",
        "outputId": "436a190c-f0e6-4bf0-f89c-646a8548c42a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([640, 480, 3])\n",
            "torch.Size([3, 640, 480])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tensor Broadcasting**"
      ],
      "metadata": {
        "id": "jo353gefqbwg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boradcasting allows you to **perform operations between a tensor and a small tensor.** You can broadcast across two tensors if, starting backward from their trailng dimensions:\n",
        "\n",
        "* The two dimensions are equal.\n",
        "* One of the dimensions is 1.\n",
        "\n"
      ],
      "metadata": {
        "id": "oEXqb34HqjCI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <center><b>Chapter 2. Image Classification with PyTorch</b></center>"
      ],
      "metadata": {
        "id": "kBAJaZs7RruC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Our Classification Problem**"
      ],
      "metadata": {
        "id": "CCrjuabbR7rh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building a simple classifier that can tell the difference between fish and cats."
      ],
      "metadata": {
        "id": "eUF4sSiBSBQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Traditional Challenges**"
      ],
      "metadata": {
        "id": "yKItcWTkSPKq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Writing a set of rules describing** that a cat has a tail, or that a fish has scales, and **apply those rules to an image to determine**."
      ],
      "metadata": {
        "id": "Xd8yhPvGSW9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need a lot of pictures of fish and cats. to train the neural network.\n",
        "\n",
        "We will use ImageNet, a standard collection of images used to train neural networks..\n",
        "\n",
        "PyTorch needs a way to determine what is a cat and what is a fish. We use a label attached to the data, and training in this manner is called supervised learning."
      ],
      "metadata": {
        "id": "qopNpXYETbeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from PIL import Image, ImageFile\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES=True"
      ],
      "metadata": {
        "id": "VaQuuXOHgWQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Specify the path to the ZIP file\n",
        "zip_path = '/content/images.zip'\n",
        "\n",
        "# Extract the contents of the ZIP file to a folder named \"images\"\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/images')"
      ],
      "metadata": {
        "id": "rwtnYFSgXIwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Loaders**"
      ],
      "metadata": {
        "id": "Wq9WiwBla3Ez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading and converting data into formats that are ready for training\n",
        "\n",
        "The two main conventions of interacting with data are datasets and data loaders.\n",
        "\n",
        "* A dataset is a Python class that allows us to get the data we're supplying to the neural network.\n",
        "* A data loader is what feeds data from the dataset into the network."
      ],
      "metadata": {
        "id": "yeqmqZWda7Di"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Building a Training Dataset, Validation and Test Datasets**"
      ],
      "metadata": {
        "id": "58LPB0sAb0Zy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`torchvision` package in cludes a class called `ImageFolder` providing our images are in a structure where each directory is label"
      ],
      "metadata": {
        "id": "Z23_yaKjb-o9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "Al_wmOUGebN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_image(path):\n",
        "    try:\n",
        "        im = Image.open(path)\n",
        "        return True\n",
        "    except:\n",
        "        return False"
      ],
      "metadata": {
        "id": "FN_xJ82MlUTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision \n",
        "from torchvision import transforms\n",
        "\n",
        "img_transforms = transforms.Compose([\n",
        "        transforms.Resize((64, 64)),                      # scale to the same resolution 64x64\n",
        "        transforms.ToTensor(),                            # take image data and turn it into a tensor\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalizing\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "735pZsIiclJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Types**\n",
        "\n",
        "* **Training set** : Used in the training pass to update the model\n",
        "* **Validation set** : Used to evaluate how the model is generalizing to the problem domain, rather than fitting to the training data; not used to update the model directly\n",
        "* **Test set** : A final dataset that provides a final evaluation of the model's performance after training is complete"
      ],
      "metadata": {
        "id": "h5eRFXCcoVCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training set\n",
        "train_data_path = \"/content/images/train\"\n",
        "train_data = torchvision.datasets.ImageFolder(root=train_data_path,\n",
        "                                              transform=img_transforms, \n",
        "                                              is_valid_file=check_image)"
      ],
      "metadata": {
        "id": "r_Bdp1-jlccx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation set\n",
        "val_data_path = \"/content/images/val\"\n",
        "val_data = torchvision.datasets.ImageFolder(root=val_data_path,\n",
        "                                            transform=img_transforms, \n",
        "                                            is_valid_file=check_image)"
      ],
      "metadata": {
        "id": "_toS3nn_cNcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test set\n",
        "test_data_path = \"/content/images/test\"\n",
        "test_data = torchvision.datasets.ImageFolder(root=test_data_path,\n",
        "                                             transform=img_transforms,\n",
        "                                             is_valid_file=check_image) "
      ],
      "metadata": {
        "id": "-wimlojAew_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`batch_size` : tell **how many images will go through the network before we train and update it**, in theory, set the `batch_size` to the number of image in the test and training sets so the network sees every image before it updates. In practice, we tend not ot do this because smaller batches (more commonly known as mini-batches in the literature) require less memory than having to store all the information about every image in the dataset, and the smaller batch size ends up making training faster as we're updating our network much more quickly.\n"
      ],
      "metadata": {
        "id": "8vyIb59c3PNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build our data loaders \n",
        "batch_size=64           \n",
        "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
        "val_data_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size)\n",
        "test_data_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "Zpl21AN6e_kA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creating a First Model, SimpleNet**"
      ],
      "metadata": {
        "id": "IjXgAQNYgLbS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SimpleNet has three linear layers and ReLu activations between them. "
      ],
      "metadata": {
        "id": "5vAap56R42zY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNet(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(SimpleNet, self).__init__()\n",
        "    self.fc1 = nn.Linear(12288, 84)\n",
        "    self.fc2 = nn.Linear(84, 50)\n",
        "    self.fc3 = nn.Linear(50, 2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, 12288)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "tzB6BHoOgOvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simplenet = SimpleNet()"
      ],
      "metadata": {
        "id": "imoxnfA9hHiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create an Optimizer**"
      ],
      "metadata": {
        "id": "h37ucwhVha9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training a network involves **passing data through the network**, using the loss function to determine the difference between prediction and the actual label, and then using that information to update the weights of the network in an attempt to make the loss function return as small a loss as possible."
      ],
      "metadata": {
        "id": "NDDnrixV68U5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we use `Adam` as our optimizer with a learning rate: `lr`, of 0.001"
      ],
      "metadata": {
        "id": "IzJrRm_85QtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(simplenet.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "_04SjAmUhdEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Copy the model to GPU**"
      ],
      "metadata": {
        "id": "tfe0RZC3n6dp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else: \n",
        "  device = torch.device(\"cpu\")\n",
        "\n",
        "simplenet.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIigdu26hnXx",
        "outputId": "50456b77-823d-4bff-b34c-9d24814efea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleNet(\n",
              "  (fc1): Linear(in_features=12288, out_features=84, bias=True)\n",
              "  (fc2): Linear(in_features=84, out_features=50, bias=True)\n",
              "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training**"
      ],
      "metadata": {
        "id": "ZJquMyorh2gh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device=\"cpu\"):\n",
        "  ### Train the model\n",
        "  for epoch in range(1, epochs+1):\n",
        "    training_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "      # Optimizer zero grad\n",
        "      optimizer.zero_grad()\n",
        "      inputs, targets = batch\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "      # Forward pass\n",
        "      output = model(inputs)\n",
        "      # Calculate loss\n",
        "      loss = loss_fn(output, targets)\n",
        "      # Loss backward (backpropagation)\n",
        "      loss.backward()\n",
        "      # Optimizer step (gradient descent)\n",
        "      optimizer.step()\n",
        "      training_loss += loss.data.item() * inputs.size(0)\n",
        "    training_loss /= len(train_loader.dataset)\n",
        "\n",
        "    ### Evaluate the model on the test set\n",
        "    model.eval()                          \n",
        "    num_correct = 0\n",
        "    num_examples = 0\n",
        "    for batch in val_loader:\n",
        "      inputs, targets = batch\n",
        "      inputs = inputs.to(device)\n",
        "      # Forward pass\n",
        "      output = model(inputs)\n",
        "      targets = targets.to(device)\n",
        "      # Calculate loss\n",
        "      loss = loss_fn(output, targets)\n",
        "      valid_loss += loss.data.item() * inputs.size(0)\n",
        "      correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets)\n",
        "      num_correct += torch.sum(correct).item()\n",
        "      num_examples += correct.shape[0] \n",
        "    valid_loss /= len(val_loader.dataset)\n",
        "\n",
        "    print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}'\n",
        "    .format(epoch, training_loss, valid_loss, num_correct / num_examples))"
      ],
      "metadata": {
        "id": "Mm__DQnMh3_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(simplenet, optimizer, torch.nn.CrossEntropyLoss(), \n",
        "      train_data_loader, val_data_loader, epochs=5, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7M3eXUOkBl5",
        "outputId": "a1f4fb52-8d1a-456f-eb59-b6374b79f288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training Loss: 1.61, Validation Loss: 7.58, accuracy = 0.22\n",
            "Epoch: 2, Training Loss: 3.03, Validation Loss: 0.94, accuracy = 0.74\n",
            "Epoch: 3, Training Loss: 0.48, Validation Loss: 2.12, accuracy = 0.36\n",
            "Epoch: 4, Training Loss: 1.01, Validation Loss: 0.83, accuracy = 0.65\n",
            "Epoch: 5, Training Loss: 0.33, Validation Loss: 1.20, accuracy = 0.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Making predictions**"
      ],
      "metadata": {
        "id": "vafdVXvyoBZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['cat','fish']\n",
        "\n",
        "img = Image.open(\"/content/images/val/fish/100_1422.JPG\") \n",
        "img = img_transforms(img).to(device)\n",
        "img = torch.unsqueeze(img, 0)\n",
        "\n",
        "simplenet.eval()\n",
        "prediction = F.softmax(simplenet(img), dim=1)\n",
        "prediction = prediction.argmax()\n",
        "print(labels[prediction]) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9O1pA5Jmt4R",
        "outputId": "37541303-0b83-476a-e679-5e602a4a0e1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Saving Models**"
      ],
      "metadata": {
        "id": "MFl1KI2GoEzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(simplenet, \"/content/simplenet\")  # to save\n",
        "simplenet = torch.load(\"/content/simplenet\")  # to load a previously saved "
      ],
      "metadata": {
        "id": "joRppbIgm4Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(simplenet.state_dict(), \"/content/simplenet\")    # save that contains the maps of each layer's parameters in the model.\n",
        "simplenet = SimpleNet()\n",
        "simplenet_state_dict = torch.load(\"/content/simplenet\")\n",
        "simplenet.load_state_dict(simplenet_state_dict)             # assigns parameters to layers in the model that do exist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLd938u8m6KS",
        "outputId": "38d5a639-a1c0-4cd8-89ba-940b91b386aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    }
  ]
}