{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "CCrjuabbR7rh",
        "IKUkk5AtgCb4",
        "sGDZaLY9kzSW",
        "Wgn1a865oZJt",
        "Zl5n_3csvoUT"
      ],
      "authorship_tag": "ABX9TyPgpbVESdYXsz5qpy5Koit1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niklaust/Deep_Learning/blob/main/PyTorch_for_Deep_Learning_notebook_of_nikluast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reference**\n",
        "Ian Pointer. (2019). *Programming PyTorch For Deep Learning Creating and Deploying Deep Learning Application*. O'Reilly"
      ],
      "metadata": {
        "id": "7sa8DyqUmEMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "github:niklaust"
      ],
      "metadata": {
        "id": "IgxpYe30mJdq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "start 20230220"
      ],
      "metadata": {
        "id": "RVRNs0e5mHr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><center><b>Programming PyTorch for Deep Learning</b></center></h1>"
      ],
      "metadata": {
        "id": "wuUZl1kumLIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What is Deep Learning?**"
      ],
      "metadata": {
        "id": "FLnNS88noYF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A machine learning technique** that **uses multiple and numerous layers of nonlinear** transforms to progressively extract features from raw input\n",
        "\n",
        "**A technique to solve problems** by providing the inputs and desired outputs and letting the computer find the solution, normally **using a neural network.**"
      ],
      "metadata": {
        "id": "HKyDFGn7oe3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <center><b>Chapter 1. Getting Started with PyTorch</b></center>"
      ],
      "metadata": {
        "id": "dSdKfmwKRIJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPU** is The **heart** of every deep learning box. It is going to **power the majority of PyTorch's calculations.**"
      ],
      "metadata": {
        "id": "ZXrpje1cSHPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Getting Start with PyTorch!** "
      ],
      "metadata": {
        "id": "rUr-hmpFWSBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.rand(2, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyIgYdeoWcBF",
        "outputId": "84e592ed-b29c-4d0d-cb71-c6570ddb5d69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "tensor([[0.8933, 0.0782],\n",
            "        [0.8609, 0.6029]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tensors**"
      ],
      "metadata": {
        "id": "Y_2-2GbbW9nK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **tensor is both a container for numbers as well as a set of rules** that define transformations between tensors that produce new tensors.\n",
        "\n",
        "It's easier to think **tensors as multidimensional arrays.**"
      ],
      "metadata": {
        "id": "nnLKPjivXDI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([[0,0,1],[1,1,1],[0,0,0]])\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsNDwSXhX-cK",
        "outputId": "3bf1828b-f0d2-420d-d649-88a1c440ebe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 1],\n",
              "        [1, 1, 1],\n",
              "        [0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change an element in a tnsor by using standard Python indexing:"
      ],
      "metadata": {
        "id": "eR9ItT5GYJKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x[0][0] = 5\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7UqJaWRYT5x",
        "outputId": "2a9bcbc5-aae0-400c-e26a-91cc526b599a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[5, 0, 1],\n",
              "        [1, 1, 1],\n",
              "        [0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.zeros(2,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HO3viBHHYcQi",
        "outputId": "a582e020-11df-43af-c550-868cd400d05f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0.],\n",
              "        [0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.ones(3,3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hFbuNfbYhws",
        "outputId": "c2281073-f474-49af-ef44-ed19e245a5ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.ones(1,2) + torch.ones(1,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tNlKw7OYn5r",
        "outputId": "84a54246-9a42-42a7-f100-9cfd7aa82136"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2., 2.]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`item`: pull out the value "
      ],
      "metadata": {
        "id": "JSSmXQ5KhRAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "torch.rand(1).item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "On458QXNg1cZ",
        "outputId": "72b04c86-cb17-4bc1-9abd-f0e0b59ff919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8822692632675171"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`to`: copy between devices "
      ],
      "metadata": {
        "id": "mUTN2b2yhLpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "XGxk_JNSiLOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cpu_tensor = torch.rand(2)\n",
        "cpu_tensor.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bgrE8X-hHrx",
        "outputId": "f487afe4-cb98-4038-cc7b-aea2d939341d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_tensor = cpu_tensor.to(\"cuda\")\n",
        "gpu_tensor.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSledL6FiPVb",
        "outputId": "c0f2deab-170d-4484-ffff-8c13d252bc42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tensor Operations**"
      ],
      "metadata": {
        "id": "kzoeMd4virvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "a = torch.rand(2,2)\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAe9z9e7jKbq",
        "outputId": "81c78925-b2c7-45f9-b896-5285aad737d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8823, 0.9150],\n",
              "        [0.3829, 0.9593]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.max())             # torch.rand(2,2).max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AY94wtXqi_ol",
        "outputId": "bd58d085-bd50-4cb6-f7f1-b29000b0595a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.9593)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.max().item())      # torch.rand(2,2).max().item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyzVyI0jjD7p",
        "outputId": "bfd60534-7af0-4769-c7d1-67e479c37e67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9593056440353394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`type`: to see the element type in the tensor\n",
        "`dtype`: to change the type of a tensor"
      ],
      "metadata": {
        "id": "88iMXEO_lBo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "long_tensor = torch.tensor([[0,0,1],[1,1,1],[0,0,0]])\n",
        "long_tensor.type()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "J1A7kuX3kl_D",
        "outputId": "11052c58-eb15-430d-ed55-8796fe0d2595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'torch.LongTensor'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "float_tensor = torch.tensor([[0,0,1],[1,1,1],[0,0,0]]).to(dtype=torch.float32)\n",
        "float_tensor.type()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nAGqJKickw7J",
        "outputId": "c3fbd4c8-d7d2-4bfd-8377-41e6fdcdd954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'torch.FloatTensor'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "appended underscore `_`: save memory, look to see if an in-place function is defined "
      ],
      "metadata": {
        "id": "MDrF3YQ-lotZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "random_tensor = torch.rand(2,2)\n",
        "random_tensor.log2()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3-Dmy3NljN5",
        "outputId": "f026d49d-f672-47fb-af14-5c8f8dd532d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1807, -0.1282],\n",
              "        [-1.3851, -0.0599]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_tensor.log2_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2TNrjv6mJnR",
        "outputId": "1510467b-3328-4e71-84ad-93292ea14b6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1807, -0.1282],\n",
              "        [-1.3851, -0.0599]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**reshape a tensor:**\n",
        "\n",
        "`view`:  **operates as a view on the original tensor**, so if the underlying data is changed, the view will change too (and vice versa). However, it can throw errors if the required view is not contiguous. It doesn't share the same block of memory it would occupy if a new tensor of  the required shape was created from scratch, you have to call `tensor.contiguous()` before you can use `view()`.\n",
        "\n",
        "`reshape`: to reshape a tensor"
      ],
      "metadata": {
        "id": "Xwp1EOOSmWbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "flat_tensor = torch.rand(784)\n",
        "print(flat_tensor.shape)                         # 1*28*28 = 784\n",
        "viewed_tensor = flat_tensor.view(1, 28, 28)\n",
        "print(viewed_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdLDyNQPmWEZ",
        "outputId": "7ce7b176-6f87-4236-8daf-f86f0a7c0dd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([784])\n",
            "torch.Size([1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "reshaped_tensor = flat_tensor.reshape(1, 28, 28)\n",
        "print(reshaped_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrmeGlVlm2EB",
        "outputId": "1d96e1a3-ffbb-48fe-c24e-30f95b7acaa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the reshpaed tensor's shape has to have the same number of \n",
        "# total elements as the original. \n",
        "\n",
        "try:\n",
        "  flat_tensor.reshape(3, 28, 28)\n",
        "except:\n",
        "  print(\"RuntimeError: shape '[3, 28, 28]' is invalid for input of size 784\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADo85p0inbmZ",
        "outputId": "8288860c-a030-4f08-9e6c-6f0d441f5aa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RuntimeError: shape '[3, 28, 28]' is invalid for input of size 784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rearrange the dimensions of a tensor.**\n",
        "\n",
        "`permute` : rearrange the dimensions of a tensor\n",
        "\n",
        "You will likely come across this with images, which often are stored as `[height, width, channel]` tensors, but PyTorch prefers to deal with these in a `[channel, height, width]` you can use `permute()` to deal with these in a fairly straightforward manner:"
      ],
      "metadata": {
        "id": "skJhMzUXpAuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "hwc_tensor = torch.rand(640, 480, 3)            # [height, width, channel]\n",
        "print(hwc_tensor.shape)\n",
        "chw_tensor = hwc_tensor.permute(2,0,1)          # [channel, height, width]\n",
        "print(chw_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npshDGfNprhx",
        "outputId": "436a190c-f0e6-4bf0-f89c-646a8548c42a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([640, 480, 3])\n",
            "torch.Size([3, 640, 480])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tensor Broadcasting**"
      ],
      "metadata": {
        "id": "jo353gefqbwg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boradcasting allows you to **perform operations between a tensor and a small tensor.** You can broadcast across two tensors if, starting backward from their trailng dimensions:\n",
        "\n",
        "* The two dimensions are equal.\n",
        "* One of the dimensions is 1.\n",
        "\n"
      ],
      "metadata": {
        "id": "oEXqb34HqjCI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <center><b>Chapter 2. Image Classification with PyTorch</b></center>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kBAJaZs7RruC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Our Classification Problem**"
      ],
      "metadata": {
        "id": "CCrjuabbR7rh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building a simple classifier that can tell the difference between fish and cats."
      ],
      "metadata": {
        "id": "eUF4sSiBSBQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Traditional Challenges**"
      ],
      "metadata": {
        "id": "yKItcWTkSPKq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Writing a set of rules describing** that a cat has a tail, or that a fish has scales, and **apply those rules to an image to determine**."
      ],
      "metadata": {
        "id": "Xd8yhPvGSW9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need a lot of pictures of fish and cats. to train the neural network.\n",
        "\n",
        "We will use ImageNet, a standard collection of images used to train neural networks..\n",
        "\n",
        "PyTorch needs a way to determine what is a cat and what is a fish. We use a label attached to the data, and training in this manner is called supervised learning."
      ],
      "metadata": {
        "id": "qopNpXYETbeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from PIL import Image, ImageFile\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES=True"
      ],
      "metadata": {
        "id": "VaQuuXOHgWQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Specify the path to the ZIP file\n",
        "zip_path = '/content/images.zip'\n",
        "\n",
        "# Extract the contents of the ZIP file to a folder named \"images\"\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/images')"
      ],
      "metadata": {
        "id": "rwtnYFSgXIwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Loaders**"
      ],
      "metadata": {
        "id": "Wq9WiwBla3Ez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading and converting data into formats that are ready for training\n",
        "\n",
        "The two main conventions of interacting with data are datasets and data loaders.\n",
        "\n",
        "* A dataset is a Python class that allows us to get the data we're supplying to the neural network.\n",
        "* A data loader is what feeds data from the dataset into the network."
      ],
      "metadata": {
        "id": "yeqmqZWda7Di"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Building a Training Dataset, Validation and Test Datasets**"
      ],
      "metadata": {
        "id": "58LPB0sAb0Zy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`torchvision` package in cludes a class called `ImageFolder` providing our images are in a structure where each directory is label"
      ],
      "metadata": {
        "id": "Z23_yaKjb-o9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "Al_wmOUGebN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_image(path):\n",
        "    try:\n",
        "        im = Image.open(path)\n",
        "        return True\n",
        "    except:\n",
        "        return False"
      ],
      "metadata": {
        "id": "FN_xJ82MlUTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision \n",
        "from torchvision import transforms\n",
        "\n",
        "img_transforms = transforms.Compose([\n",
        "        transforms.Resize((64, 64)),                      # scale to the same resolution 64x64\n",
        "        transforms.ToTensor(),                            # take image data and turn it into a tensor\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalizing\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "735pZsIiclJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Types**\n",
        "\n",
        "* **Training set** : Used in the training pass to update the model\n",
        "* **Validation set** : Used to evaluate how the model is generalizing to the problem domain, rather than fitting to the training data; not used to update the model directly\n",
        "* **Test set** : A final dataset that provides a final evaluation of the model's performance after training is complete"
      ],
      "metadata": {
        "id": "h5eRFXCcoVCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training set\n",
        "train_data_path = \"/content/images/train\"\n",
        "train_data = torchvision.datasets.ImageFolder(root=train_data_path,\n",
        "                                              transform=img_transforms, \n",
        "                                              is_valid_file=check_image)"
      ],
      "metadata": {
        "id": "r_Bdp1-jlccx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation set\n",
        "val_data_path = \"/content/images/val\"\n",
        "val_data = torchvision.datasets.ImageFolder(root=val_data_path,\n",
        "                                            transform=img_transforms, \n",
        "                                            is_valid_file=check_image)"
      ],
      "metadata": {
        "id": "_toS3nn_cNcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test set\n",
        "test_data_path = \"/content/images/test\"\n",
        "test_data = torchvision.datasets.ImageFolder(root=test_data_path,\n",
        "                                             transform=img_transforms,\n",
        "                                             is_valid_file=check_image) "
      ],
      "metadata": {
        "id": "-wimlojAew_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`batch_size` : tell **how many images will go through the network before we train and update it**, in theory, set the `batch_size` to the number of image in the test and training sets so the network sees every image before it updates. In practice, we tend not ot do this because smaller batches (more commonly known as mini-batches in the literature) require less memory than having to store all the information about every image in the dataset, and the smaller batch size ends up making training faster as we're updating our network much more quickly.\n"
      ],
      "metadata": {
        "id": "8vyIb59c3PNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build our data loaders \n",
        "batch_size=64           \n",
        "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
        "val_data_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size)\n",
        "test_data_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "Zpl21AN6e_kA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creating a First Model, SimpleNet**"
      ],
      "metadata": {
        "id": "IjXgAQNYgLbS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SimpleNet has three linear layers and ReLu activations between them. "
      ],
      "metadata": {
        "id": "5vAap56R42zY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNet(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(SimpleNet, self).__init__()\n",
        "    self.fc1 = nn.Linear(12288, 84)\n",
        "    self.fc2 = nn.Linear(84, 50)\n",
        "    self.fc3 = nn.Linear(50, 2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, 12288)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "tzB6BHoOgOvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simplenet = SimpleNet()"
      ],
      "metadata": {
        "id": "imoxnfA9hHiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create an Optimizer**"
      ],
      "metadata": {
        "id": "h37ucwhVha9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training a network involves **passing data through the network**, using the loss function to determine the difference between prediction and the actual label, and then using that information to update the weights of the network in an attempt to make the loss function return as small a loss as possible."
      ],
      "metadata": {
        "id": "NDDnrixV68U5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we use `Adam` as our optimizer with a learning rate: `lr`, of 0.001"
      ],
      "metadata": {
        "id": "IzJrRm_85QtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(simplenet.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "_04SjAmUhdEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Copy the model to GPU**"
      ],
      "metadata": {
        "id": "tfe0RZC3n6dp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else: \n",
        "  device = torch.device(\"cpu\")\n",
        "\n",
        "simplenet.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIigdu26hnXx",
        "outputId": "50456b77-823d-4bff-b34c-9d24814efea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleNet(\n",
              "  (fc1): Linear(in_features=12288, out_features=84, bias=True)\n",
              "  (fc2): Linear(in_features=84, out_features=50, bias=True)\n",
              "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training**"
      ],
      "metadata": {
        "id": "ZJquMyorh2gh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device=\"cpu\"):\n",
        "  ### Train the model\n",
        "  for epoch in range(1, epochs+1):\n",
        "    training_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "      # Optimizer zero grad\n",
        "      optimizer.zero_grad()\n",
        "      inputs, targets = batch\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "      # Forward pass\n",
        "      output = model(inputs)\n",
        "      # Calculate loss\n",
        "      loss = loss_fn(output, targets)\n",
        "      # Loss backward (backpropagation)\n",
        "      loss.backward()\n",
        "      # Optimizer step (gradient descent)\n",
        "      optimizer.step()\n",
        "      training_loss += loss.data.item() * inputs.size(0)\n",
        "    training_loss /= len(train_loader.dataset)\n",
        "\n",
        "    ### Evaluate the model on the test set\n",
        "    model.eval()                          \n",
        "    num_correct = 0\n",
        "    num_examples = 0\n",
        "    for batch in val_loader:\n",
        "      inputs, targets = batch\n",
        "      inputs = inputs.to(device)\n",
        "      # Forward pass\n",
        "      output = model(inputs)\n",
        "      targets = targets.to(device)\n",
        "      # Calculate loss\n",
        "      loss = loss_fn(output, targets)\n",
        "      valid_loss += loss.data.item() * inputs.size(0)\n",
        "      correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets)\n",
        "      num_correct += torch.sum(correct).item()\n",
        "      num_examples += correct.shape[0] \n",
        "    valid_loss /= len(val_loader.dataset)\n",
        "\n",
        "    print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}'\n",
        "    .format(epoch, training_loss, valid_loss, num_correct / num_examples))"
      ],
      "metadata": {
        "id": "Mm__DQnMh3_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(simplenet, optimizer, torch.nn.CrossEntropyLoss(), \n",
        "      train_data_loader, val_data_loader, epochs=5, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7M3eXUOkBl5",
        "outputId": "a1f4fb52-8d1a-456f-eb59-b6374b79f288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training Loss: 1.61, Validation Loss: 7.58, accuracy = 0.22\n",
            "Epoch: 2, Training Loss: 3.03, Validation Loss: 0.94, accuracy = 0.74\n",
            "Epoch: 3, Training Loss: 0.48, Validation Loss: 2.12, accuracy = 0.36\n",
            "Epoch: 4, Training Loss: 1.01, Validation Loss: 0.83, accuracy = 0.65\n",
            "Epoch: 5, Training Loss: 0.33, Validation Loss: 1.20, accuracy = 0.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Making predictions**"
      ],
      "metadata": {
        "id": "vafdVXvyoBZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`unsqueeze` adds a new dimension at the front of our tensor.\n",
        "\n",
        "`argmax` returns the index of the highest values of the tensor.\n",
        "\n",
        "``"
      ],
      "metadata": {
        "id": "cQDvEEIjaZ2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['cat','fish']\n",
        "\n",
        "img = Image.open(\"/content/images/val/fish/100_1422.JPG\") \n",
        "img = img_transforms(img).to(device)\n",
        "img = torch.unsqueeze(img, 0)\n",
        "\n",
        "simplenet.eval()\n",
        "prediction = F.softmax(simplenet(img), dim=1)\n",
        "prediction = prediction.argmax()      \n",
        "print(labels[prediction]) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9O1pA5Jmt4R",
        "outputId": "37541303-0b83-476a-e679-5e602a4a0e1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Saving Models**"
      ],
      "metadata": {
        "id": "MFl1KI2GoEzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(simplenet, \"/content/simplenet\")  # to save\n",
        "simplenet = torch.load(\"/content/simplenet\")  # to load a previously saved "
      ],
      "metadata": {
        "id": "joRppbIgm4Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(simplenet.state_dict(), \"/content/simplenet\")    # save that contains the maps of each layer's parameters in the model.\n",
        "simplenet = SimpleNet()\n",
        "simplenet_state_dict = torch.load(\"/content/simplenet\")\n",
        "simplenet.load_state_dict(simplenet_state_dict)             # assigns parameters to layers in the model that do exist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLd938u8m6KS",
        "outputId": "38d5a639-a1c0-4cd8-89ba-940b91b386aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <center><b>Chapter 3. Convolutional Neural Networks</b></center>\n"
      ],
      "metadata": {
        "id": "OvuqVCREbFCy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional neural networks (CNNs) is the backbone of the most accurate image classifiers around today."
      ],
      "metadata": {
        "id": "P8Ovr_-scAta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Our First Convolutional Model**"
      ],
      "metadata": {
        "id": "3p6VYY55cRUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`nn.Sequential()` allows us to create a chain of layers. When we use one of these chains in `forward()`, the input goes through each element of the array of layers in succession. You can use this to break your model into more logical arrangements. \n",
        "\n",
        "In this network, we have two chains: the `features` block and the `classifier`."
      ],
      "metadata": {
        "id": "kofUuGqFfQLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "aL6yb0PgcrO5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extract image file**"
      ],
      "metadata": {
        "id": "rUSi7PUSFgfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Specify the path to the ZIP file\n",
        "zip_path = '/content/images.zip'\n",
        "\n",
        "# Extract the contents of the ZIP file to a folder named \"images\"\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/images')"
      ],
      "metadata": {
        "id": "HIk535PYCnMJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNNNet (or AlexNet)**"
      ],
      "metadata": {
        "id": "Gykd5rz8Aa2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNNet(nn.Module):\n",
        "\n",
        "  def __init__(self, num_classes=2):\n",
        "    super(CNNNet, self).__init__()\n",
        "    self.features = nn.Sequential(\n",
        "        nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),   \n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        nn.Conv2d(64, 192, kernel_size=5, padding=2),       \n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        nn.Conv2d(192, 384, kernel_size=3, padding=1),           \n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(384, 256, kernel_size=3, padding=1),          \n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "    )\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(256 * 6 * 6, 4096),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(4096, 4096),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4096, num_classes)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    x = self.avgpool(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "uECf4wBfcQzt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnnnet = CNNNet()"
      ],
      "metadata": {
        "id": "5s3-KQkVfH8o"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create train function**"
      ],
      "metadata": {
        "id": "z7ald9GJAYNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device=\"cpu\"):\n",
        "  ### Train the model\n",
        "  for epoch in range(1, epochs+1):\n",
        "    training_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "      # Optimizer zero grad\n",
        "      optimizer.zero_grad()\n",
        "      inputs, targets = batch\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "      # Forward pass\n",
        "      output = model(inputs)\n",
        "      # Calculate loss\n",
        "      loss = loss_fn(output, targets)\n",
        "      # Loss backward (backpropagation)\n",
        "      loss.backward()\n",
        "      # Optimizer step (gradient descent)\n",
        "      optimizer.step()\n",
        "      training_loss += loss.data.item() * inputs.size(0)\n",
        "    training_loss /= len(train_loader.dataset)\n",
        "\n",
        "    ### Evaluate the model on the test set\n",
        "    model.eval()                          \n",
        "    num_correct = 0\n",
        "    num_examples = 0\n",
        "    for batch in val_loader:\n",
        "      inputs, targets = batch\n",
        "      inputs = inputs.to(device)\n",
        "      # Forward pass\n",
        "      output = model(inputs)\n",
        "      targets = targets.to(device)\n",
        "      # Calculate loss\n",
        "      loss = loss_fn(output, targets)\n",
        "      valid_loss += loss.data.item() * inputs.size(0)\n",
        "      correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets)\n",
        "      num_correct += torch.sum(correct).item()\n",
        "      num_examples += correct.shape[0] \n",
        "    valid_loss /= len(val_loader.dataset)\n",
        "\n",
        "    print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}'\n",
        "    .format(epoch, training_loss, valid_loss, num_correct / num_examples))"
      ],
      "metadata": {
        "id": "NaP7-QwiAXVy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check image**"
      ],
      "metadata": {
        "id": "IEtV8RShAxYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_image(path):\n",
        "    try:\n",
        "        im = Image.open(path)\n",
        "        return True\n",
        "    except:\n",
        "        return False"
      ],
      "metadata": {
        "id": "-PvdsPvkAqoS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resize and Transform to tensor**"
      ],
      "metadata": {
        "id": "vHNjip6BAz7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_transforms = transforms.Compose([\n",
        "        transforms.Resize((64, 64)),                      # scale to the same resolution 64x64\n",
        "        transforms.ToTensor(),                            # take image data and turn it into a tensor\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalizing\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "uS-HIek7Av5y"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training set\n",
        "train_data_path = \"/content/images/train\"\n",
        "train_data = torchvision.datasets.ImageFolder(root=train_data_path,\n",
        "                                              transform=img_transforms, \n",
        "                                              is_valid_file=check_image)\n",
        "\n",
        "# Validation set\n",
        "val_data_path = \"/content/images/val\"\n",
        "val_data = torchvision.datasets.ImageFolder(root=val_data_path,\n",
        "                                            transform=img_transforms, \n",
        "                                            is_valid_file=check_image)\n",
        "\n",
        "# Test set\n",
        "test_data_path = \"/content/images/test\"\n",
        "test_data = torchvision.datasets.ImageFolder(root=test_data_path,\n",
        "                                             transform=img_transforms,\n",
        "                                             is_valid_file=check_image) "
      ],
      "metadata": {
        "id": "gWbO-fOYA_gE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build dataloader**"
      ],
      "metadata": {
        "id": "x_d5QPaEB1o0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=64           \n",
        "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
        "val_data_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size)\n",
        "test_data_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "t0GQpkCjBz_R"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Copy model to GPU**"
      ],
      "metadata": {
        "id": "-E1uCwm6BMIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else: \n",
        "  device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "lDMCq9HkBMlP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clibx3m5FXrH",
        "outputId": "ff615e84-c26c-47bd-db1d-77777077c04b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Copy model to device and Create optimizer**"
      ],
      "metadata": {
        "id": "9bMpNtrwBXAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnnnet.to(device)\n",
        "optimizer = optim.Adam(cnnnet.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "-P4aXUJvBYD8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train**"
      ],
      "metadata": {
        "id": "jQTqPcYUCBfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(cnnnet, optimizer, torch.nn.CrossEntropyLoss(), train_data_loader, val_data_loader, epochs=10, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpN4CIeUCLlZ",
        "outputId": "fe32c225-950e-4fa1-8316-caa6d8dccd5e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training Loss: 419.04, Validation Loss: 0.71, accuracy = 0.18\n",
            "Epoch: 2, Training Loss: 0.70, Validation Loss: 0.72, accuracy = 0.18\n",
            "Epoch: 3, Training Loss: 0.70, Validation Loss: 0.72, accuracy = 0.18\n",
            "Epoch: 4, Training Loss: 0.69, Validation Loss: 0.73, accuracy = 0.18\n",
            "Epoch: 5, Training Loss: 0.69, Validation Loss: 0.74, accuracy = 0.18\n",
            "Epoch: 6, Training Loss: 0.69, Validation Loss: 0.75, accuracy = 0.18\n",
            "Epoch: 7, Training Loss: 0.68, Validation Loss: 0.77, accuracy = 0.18\n",
            "Epoch: 8, Training Loss: 0.69, Validation Loss: 0.77, accuracy = 0.18\n",
            "Epoch: 9, Training Loss: 0.68, Validation Loss: 0.78, accuracy = 0.18\n",
            "Epoch: 10, Training Loss: 0.69, Validation Loss: 0.79, accuracy = 0.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Convolutions**"
      ],
      "metadata": {
        "id": "IKUkk5AtgCb4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Conv2d` layer is a 2D convolution. If we have a grayscale image, it consists of an array, x pixel wide and y pixels high, with each entry having a value that indicates whether it's balck or white or some where in between\n",
        "\n",
        "    nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "\n",
        "`in_channels` is the number of input channels\n",
        "\n",
        "`out_channels` is the number of output channels, which corresponds to the number of filters in our conv layer.\n",
        "\n",
        "`kernel_size` describes the height and width of our filter. \n",
        "\n",
        "`stride` indicates how many steps across the input we move when we adjust the filter to a new position. \n",
        "\n",
        "`padding` set edge "
      ],
      "metadata": {
        "id": "88bHn5txgTMQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pooling**"
      ],
      "metadata": {
        "id": "sGDZaLY9kzSW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These layers **reduce the resolution** of the network from the previous input layer, which gives us fewer parameters in lower layers.\n",
        "\n",
        "In our model, we're using `MaxPool2d`\n",
        "\n",
        "`MaxPool` we take the maximum value from each of these tensors.\n",
        "\n",
        "A popular alternative is `AvgPool`, take the average of the tensor values. \n"
      ],
      "metadata": {
        "id": "u9hWxgYRlC_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dropout**"
      ],
      "metadata": {
        "id": "Wgn1a865oZJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Dropout`layer allows networks to learn and generalize to nontraining data without simply learning how to just respond to the training inputs."
      ],
      "metadata": {
        "id": "TKdT8lspojP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **History of CNN Architectures**"
      ],
      "metadata": {
        "id": "hHvk3_tfpgO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LeNet-5** 1990's used for digit recognition on check\n",
        " "
      ],
      "metadata": {
        "id": "UzBccnAnqTE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AlexNet** in 2012, one of the first architectures to introduce the concepts of `MaxPool` and `Dropout`. It was one of the first archituectures to demonstrate that many layers were possible and efficient to train on a GPU."
      ],
      "metadata": {
        "id": "kH0vjWxOq0DD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inception/GoogLeNet** in 2014, the GoogLeNet architecture introduced the Inception module that addressed some of the deficiencies of AlexNet. In that networks, the **kernels of the convolutional layers are fixed** at a certain resolution. \n",
        "\n",
        "The inception network instead **runs a series of convolutions of different sizes** all on the same input, and concatenates all of the filters together to pass on to the next layer."
      ],
      "metadata": {
        "id": "JGTeOtkzrnoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VGG** in 2014. VGG is a **simpler stack of convolutional layers**. Coming in various configurations of longer stacks of convolutional filters **combined with two large hidden linear layers** before the final classificaiton layer.\n",
        "\n",
        "**The downsize** of the VGG approach is the final fully connected layers make the network balloon to  **a large size**"
      ],
      "metadata": {
        "id": "ue_TjE32s6PK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ResNet** in 2015, **Stacking bundle of layers approach**, wherein each bundle performed the usual CNN operations but also added the incoming input to the output of the block.\n",
        "\n",
        "The advantage of this set up is that each block passes through the original input to the next layer, allowing the **\"signal\" of the training data to traverse through deeper networks** than possible in either VGG or Inception."
      ],
      "metadata": {
        "id": "U8F42PZWuNha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Using Pretrained Models in PyTorch**"
      ],
      "metadata": {
        "id": "Zl5n_3csvoUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch provides many of the most popular models by default in the `torchvision` library. For AlexNet,"
      ],
      "metadata": {
        "id": "w4c2yaDWvtlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import trochvision.models as models\n",
        "\n",
        "alexnet = models.alexnet(num_classes=2)"
      ],
      "metadata": {
        "id": "M4hj-GkKv9AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Examining a Model's Structure**"
      ],
      "metadata": {
        "id": "7K9YYakawZWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "model = models.resnet18()"
      ],
      "metadata": {
        "id": "8-BUOTkg9CE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)      # show model structure"
      ],
      "metadata": {
        "id": "4idvJLasFLsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "uPfD7206DIVo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, optimizer, torch.nn.CrossEntropyLoss(), train_data_loader, val_data_loader, epochs=10, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iloYJWXDOb5",
        "outputId": "9a7a781f-6267-4be3-ac1e-c3e9ee1915be"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training Loss: 4.62, Validation Loss: 5.69, accuracy = 0.18\n",
            "Epoch: 2, Training Loss: 1.15, Validation Loss: 2.26, accuracy = 0.18\n",
            "Epoch: 3, Training Loss: 1.17, Validation Loss: 1.14, accuracy = 0.18\n",
            "Epoch: 4, Training Loss: 0.86, Validation Loss: 1.14, accuracy = 0.18\n",
            "Epoch: 5, Training Loss: 0.87, Validation Loss: 0.97, accuracy = 0.18\n",
            "Epoch: 6, Training Loss: 0.85, Validation Loss: 0.95, accuracy = 0.18\n",
            "Epoch: 7, Training Loss: 0.81, Validation Loss: 0.92, accuracy = 0.18\n",
            "Epoch: 8, Training Loss: 0.78, Validation Loss: 0.90, accuracy = 0.18\n",
            "Epoch: 9, Training Loss: 0.73, Validation Loss: 0.88, accuracy = 0.19\n",
            "Epoch: 10, Training Loss: 0.64, Validation Loss: 0.95, accuracy = 0.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BatchNorm**"
      ],
      "metadata": {
        "id": "CjuCdmOr9x3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BatchNorm**, short for batch normalization, is a layer commonly used in deep neural networks. Its **main purpose is to normalize the activations of a previous layer**, ensuring that each mini-batch that goes through the network has a mean centered around zero and a variance of 1. This is achieved by using two learned parameters that are trained along with the rest of the network.\n",
        "\n",
        "The importance of BatchNorm becomes more apparent as networks get larger. **With repeated multiplication,** the effect of any layer on another layer further down the network can become significant. This can lead to vanishing or exploding gradients, which can make training the network impossible. The BatchNorm layer helps prevent these issues by keeping the activations of each layer within a stable range. Therefore, even in very deep networks like ResNet-152, the BatchNorm layer helps ensure that the network's multiplications do not get out of hand."
      ],
      "metadata": {
        "id": "Li-wNjif90MH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Downloading a pretrained network**"
      ],
      "metadata": {
        "id": "e_yoMWFmE9so"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AlexNet**"
      ],
      "metadata": {
        "id": "uqfhVC9QHoq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models"
      ],
      "metadata": {
        "id": "rq6LqwRLEHva"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alexnet = models.alexnet(num_classes=1000, weights='AlexNet_Weights.IMAGENET1K_V1')"
      ],
      "metadata": {
        "id": "MI7lUi08I-H4"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(alexnet)"
      ],
      "metadata": {
        "id": "qTl08UxqEkgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alexnet.to(device)\n",
        "optimizer = optim.Adam(alexnet.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "yMmYN4d0Exan"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(alexnet, optimizer, torch.nn.CrossEntropyLoss(), train_data_loader, val_data_loader, epochs=10, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gV78t6KDE1-Q",
        "outputId": "143778b9-e897-4cd6-b873-8fc41b1c564b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training Loss: 40.72, Validation Loss: 84.87, accuracy = 0.18\n",
            "Epoch: 2, Training Loss: 17.33, Validation Loss: 1.29, accuracy = 0.18\n",
            "Epoch: 3, Training Loss: 1.07, Validation Loss: 1.44, accuracy = 0.18\n",
            "Epoch: 4, Training Loss: 1.09, Validation Loss: 1.08, accuracy = 0.18\n",
            "Epoch: 5, Training Loss: 0.91, Validation Loss: 1.12, accuracy = 0.18\n",
            "Epoch: 6, Training Loss: 0.89, Validation Loss: 1.04, accuracy = 0.18\n",
            "Epoch: 7, Training Loss: 0.85, Validation Loss: 1.00, accuracy = 0.18\n",
            "Epoch: 8, Training Loss: 0.81, Validation Loss: 1.01, accuracy = 0.18\n",
            "Epoch: 9, Training Loss: 0.81, Validation Loss: 0.98, accuracy = 0.18\n",
            "Epoch: 10, Training Loss: 0.79, Validation Loss: 0.96, accuracy = 0.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GoogLeNet**"
      ],
      "metadata": {
        "id": "Sr3Y6OVdHc4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "googlenet = models.googlenet(num_classes=1000, weights='GoogLeNet_Weights.IMAGENET1K_V1')"
      ],
      "metadata": {
        "id": "fB8dFe5bHgGH"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(googlenet)"
      ],
      "metadata": {
        "id": "DARzHV_tHif-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "googlenet.to(device)\n",
        "optimizer = optim.Adam(googlenet.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "QrujTCEBHliW"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(googlenet, optimizer, torch.nn.CrossEntropyLoss(), train_data_loader, val_data_loader, epochs=10, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dm56mprHIk4d",
        "outputId": "73b1d1ee-3a34-4354-8a2b-890ab5a4f42c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training Loss: 6.51, Validation Loss: 1.56, accuracy = 0.69\n",
            "Epoch: 2, Training Loss: 1.86, Validation Loss: 1.38, accuracy = 0.50\n",
            "Epoch: 3, Training Loss: 0.92, Validation Loss: 1.00, accuracy = 0.33\n",
            "Epoch: 4, Training Loss: 0.91, Validation Loss: 0.87, accuracy = 0.32\n",
            "Epoch: 5, Training Loss: 0.63, Validation Loss: 1.41, accuracy = 0.24\n",
            "Epoch: 6, Training Loss: 0.60, Validation Loss: 1.19, accuracy = 0.21\n",
            "Epoch: 7, Training Loss: 0.46, Validation Loss: 0.93, accuracy = 0.39\n",
            "Epoch: 8, Training Loss: 0.31, Validation Loss: 0.92, accuracy = 0.41\n",
            "Epoch: 9, Training Loss: 0.22, Validation Loss: 1.06, accuracy = 0.35\n",
            "Epoch: 10, Training Loss: 0.10, Validation Loss: 1.05, accuracy = 0.36\n"
          ]
        }
      ]
    }
  ]
}